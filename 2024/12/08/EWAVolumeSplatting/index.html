<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>EWA Volume Splatting | Zhu Low Key</title><meta name="author" content="Zhu Jiajun"><meta name="copyright" content="Zhu Jiajun"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="elliptical Gaussian kernels 椭圆高斯核 Splatting algorithms interpret volume data as a set of particles that are absorbing and emitting light. 泼溅算法将体素数据解释为一组吸收和发射光的粒子。 Our method is based on a novel framew">
<meta property="og:type" content="article">
<meta property="og:title" content="EWA Volume Splatting">
<meta property="og:url" content="http://example.com/2024/12/08/EWAVolumeSplatting/index.html">
<meta property="og:site_name" content="Zhu Low Key">
<meta property="og:description" content="elliptical Gaussian kernels 椭圆高斯核 Splatting algorithms interpret volume data as a set of particles that are absorbing and emitting light. 泼溅算法将体素数据解释为一组吸收和发射光的粒子。 Our method is based on a novel framew">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/EWA1.png">
<meta property="article:published_time" content="2024-12-08T13:58:49.000Z">
<meta property="article:modified_time" content="2024-12-08T14:36:37.327Z">
<meta property="article:author" content="Zhu Jiajun">
<meta property="article:tag" content="三维重建">
<meta property="article:tag" content="3D高斯预备知识">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/EWA1.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "EWA Volume Splatting",
  "url": "http://example.com/2024/12/08/EWAVolumeSplatting/",
  "image": "http://example.com/img/EWA1.png",
  "datePublished": "2024-12-08T13:58:49.000Z",
  "dateModified": "2024-12-08T14:36:37.327Z",
  "author": [
    {
      "@type": "Person",
      "name": "Zhu Jiajun",
      "url": "http://example.com/"
    }
  ]
}</script><link rel="shortcut icon" href="https://cdn.icon-icons.com/icons2/1804/PNG/512/iconfinder-517-astronomy-plenet-education-learning-4212897_114939.png"><link rel="canonical" href="http://example.com/2024/12/08/EWAVolumeSplatting/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"已切换为繁体中文","cht_to_chs":"已切换为简体中文","day_to_night":"已切换为深色模式","night_to_day":"已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-center"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'EWA Volume Splatting',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/myphoto.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">58</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">37</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/EWA1.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="/img/logo.png" alt="Logo"><span class="site-name">Zhu Low Key</span></a><a class="nav-page-title" href="/"><span class="site-name">EWA Volume Splatting</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">EWA Volume Splatting</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-12-08T13:58:49.000Z" title="发表于 2024-12-08 21:58:49">2024-12-08</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-08T14:36:37.327Z" title="更新于 2024-12-08 22:36:37">2024-12-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/">三维重建</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">2.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>13分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>elliptical Gaussian kernels 椭圆高斯核</p>
<p>Splatting algorithms interpret volume data as a set of particles that are absorbing and emitting light. 泼溅算法将体素数据解释为一组吸收和发射光的粒子。</p>
<p>Our method is based on a novel framework to compute the footprint function, which relies on the transformation of the volume data to so-called ray space. This transformation is equivalent to perspective projection. By using its local affine approximation at each voxel, we derive an analytic expression for the EWA footprint in screen space. The rasterization of the footprint is performed using forward differencing requiring only one 1D footprint table for all reconstruction kernels and any viewing direction.”</p>
<p>我们的方法基于一个新的框架来计算足迹函数，该框架依赖于将体积数据转换为所谓的射线空间。这种转换相当于透视投影。通过在每个体素处使用其局部仿射近似，我们推导出了屏幕空间中EWA足迹的解析表达式。足迹的光栅化是使用正向差分进行的，只需要一个1D足迹表用于所有重建内核和任何观察方向。</p>
<p><img src="/img/EWA1.png" alt="EWA1" title="EWA1"></p>
<h2 id="Splatting-Algorithms"><a href="#Splatting-Algorithms" class="headerlink" title="Splatting Algorithms"></a>Splatting Algorithms</h2><p><img src="/img/EWA2.png" alt="EWA2" title="EWA2"></p>
<p>a point in ray space:</p>
<script type="math/tex; mode=display">
\mathbf{x}=\left( x_{0}, x_{1}, x_{2} \right)^{T}</script><p>The coordinates $x_0$ and $x_1$ specify a point on the projection plane.</p>
<p>The ray intersecting the center of projection and the point $(x_0,x_1)$ on the projection plane is called a viewing ray.</p>
<script type="math/tex; mode=display">
\hat{\mathbf{x}}=( x_{0}, x_{1} )^{T}</script><p>The third coordinate $x_2$ specifies the Euclidean distance from the center of projection to a point on the viewing ray.</p>
<p>The volume rendering equation:</p>
<script type="math/tex; mode=display">
I_{\lambda} ( \hat{\bf x} )=\int_{0}^{L} c_{\lambda} ( \hat{\bf x}, \xi) g ( \hat{\bf x}, \xi) e^{-\int_{0}^{\xi} g ( \hat{\bf x}, \mu) \, d \mu} \, d \xi, \tag{1}</script><p>$g(x)$ is the extinction function that defines the rate of light occlusion, and $c_λ(x)$ is an emission coefficient.</p>
<p>$c_λ(x)g(x)$ is also called the source term,describing the light intensity scattered in the direction of the ray  $x$ at the point $x_2$.</p>
<script type="math/tex; mode=display">
g ( \mathbf{x} )=\sum_{k} g_{k} r_{k} ( \mathbf{x} ). \tag{2}</script><p>coefficients $g_k$ and reconstruction kernels $r_k(x)$</p>
<script type="math/tex; mode=display">
I_{\lambda} ( \hat{\bf x} )=\sum_{k} \Bigg( \int_{0}^{L} c_{\lambda} ( \hat{\bf x}, \xi) g_{k} r_{k} ( \hat{\bf x}, \xi) \, \prod_{j} e^{-g_{j} \int_{0}^{\xi} r_{j} ( \hat{\bf x}, \mu) \, d \mu} \, d \xi\Bigg). \tag{3}</script><p>We also assume that the emission coefficient is constant in the support of each reconstruction kernel along a ray, hence we use the notation $c_{λk}(x)=c_λ(x,x_2)$</p>
<p>approximate the exponential function with the first two terms of its Taylor expansion, thus $e^x≈ 1−x$.</p>
<script type="math/tex; mode=display">
I_{\lambda} ( \hat{\mathbf{x}} )=\sum_{k} c_{\lambda k} ( \hat{\mathbf{x}} ) g_{k} q_{k} ( \hat{\mathbf{x}} ) \prod_{j=0}^{k-1} \left( 1-g_{j} q_{j} ( \hat{\mathbf{x}} ) \right), \tag{4}</script><script type="math/tex; mode=display">
q_{k} ( \hat{\mathbf{x}} )=\int_{\mathbb{R}} r_{k} ( \hat{\mathbf{x}}, x_{2} ) \, d x_{2}. \tag{5}</script><h2 id="The-EWA-Volume-Resampling-Filter"><a href="#The-EWA-Volume-Resampling-Filter" class="headerlink" title="The EWA Volume Resampling Filter"></a>The EWA Volume Resampling Filter</h2><h3 id="Aliasing-in-Volume-Splatting"><a href="#Aliasing-in-Volume-Splatting" class="headerlink" title="Aliasing in Volume Splatting"></a>Aliasing in Volume Splatting</h3><p>achieve this band-limitation by convolving $ I_λ(\hat{x})$ with an appropriate low-pass filter $ h(\hat{x})$, yielding the antialiased splatting equation:</p>
<script type="math/tex; mode=display">
( I_{\lambda} \otimes h ) ( \hat{\bf x} )=\int_{\mathbb{R}^{2}} \sum_{k} c_{\lambda k} ( \eta) g_{k} q_{k} ( \eta) \, \prod_{j=0}^{k-1} \left( 1-g_{j} q_{j} ( \eta) \right) h ( \hat{\bf x}-\eta) \, d \eta. \tag{6}</script><p>First, we assume that the emission coefficient is approximately constant in the support of each footprint function $q_k$, hence $c_λk (\hat{x}) ≈ c_λk$ for all $\hat{x}$ in the support area.</p>
<p>Together with the assumption that the emission coefficient is constant in the support of each reconstruction kernel along a viewing ray, this means that the emission coefficient is constant in the complete 3D support of each reconstruction kernel.</p>
<p>Additionally, we assume that the attenuation factor has an approximately constant value $o_k$ in the support of each footprint function:</p>
<script type="math/tex; mode=display">
\prod_{j=0}^{k-1} \left( 1-g_{j} q_{j} ( \hat{\mathbf{x}} ) \right) \approx o_{k} \tag{7}</script><script type="math/tex; mode=display">
\begin{array} {r c l} ( I_{\lambda} \otimes h ) ( \hat{\bf x} ) & \approx & \sum_{k} c_{\lambda k} o_{k} g_{k} \int_{\mathbb{R}^{2}} q_{k} ( \eta) h ( \hat{\bf x}-\eta) \, d \eta \\  & = & \sum_{k} c_{\lambda k} o_{k} g_{k} ( q_{k} \otimes h ) ( \hat{\bf x} ). \\ \end{array}</script><p>ideal resampling filter,footprint function $q_k$ and a low-pass kernel $h$</p>
<script type="math/tex; mode=display">
\rho_{k} ( \hat{\bf x} )=( q_{k} \otimes h ) ( \hat{\bf x} ) \tag{8}</script><p>This means that instead of band-limiting the output function  directly, we band-limit each footprint function separately.</p>
<h3 id="Elliptical-Gaussian-Kernels"><a href="#Elliptical-Gaussian-Kernels" class="headerlink" title="Elliptical Gaussian Kernels"></a>Elliptical Gaussian Kernels</h3><p>We define an elliptical Gaussian $G_V(x − p)$ centered at a point $p=(p_0,p_1,p_2)^T$ with a variance matrix $V$ as:</p>
<script type="math/tex; mode=display">
\mathcal{G}_{\mathbf{V}} ( \mathbf{x}-\mathbf{p} )=\frac{1} {2 \pi| \mathbf{V} |^{\frac{1} {2}}} e^{-\frac{1} {2} ( \mathbf{x}-\mathbf{p} )^{T} \mathbf{V}^{-1} ( \mathbf{x}-\mathbf{p} )}, \tag{9}</script><p>$x=(x_0,x_1,x_2)^T$</p>
<p>the affine mapping as $Φ(x)= Mx + c$, where $M$ is a 3 × 3 matrix and $c$ is a vector $(c_0,c_1,c_2)^T$ . We substitute $x =Φ^{−1}(u)$ in (9), yielding:</p>
<script type="math/tex; mode=display">
\mathcal{G}_{\mathbf{V}} ( \Phi^{-1} ( \mathbf{u} )-\mathbf{p} )=\frac{1} {| \mathbf{M}^{-1} |} \mathcal{G}_{\mathbf{M V M}^{T}} ( \mathbf{u}-\Phi( \mathbf{p} ) ). \tag{10}</script><p>convolving two Gaussians with variance matrices $\bf{V}$ and $\bf{Y}$ results in another Gaussian with variance matrix $\bf{V + Y}$:</p>
<script type="math/tex; mode=display">
( \mathcal{G}_{\mathbf{V}} \otimes\mathcal{G}_{\mathbf{Y}} ) ( \mathbf{x}-\mathbf{p} )=\mathcal{G}_{\mathbf{V}+\mathbf{Y}} ( \mathbf{x}-\mathbf{p} ). \tag{11}</script><p>integrating a 3D Gaussian $\mathcal{G}_V$ along one coordinate axis yields a 2D Gaussian $\mathcal{G}_{\hat{V}}$, hence:</p>
<script type="math/tex; mode=display">
\int_{\mathbb{R}} \mathcal{G}_{\mathbf{V}} ( \mathbf{x}-\mathbf{p} ) \, d x_{2}=\mathcal{G}_{\hat{\mathbf{V}}} ( \hat{\mathbf{x}}-\hat{\mathbf{p}} ), \tag{12}</script><p>where $\hat{\bf{x}} =(x_0,x_1)^T$and $\hat{\bf{p}} =(p_0,p_1)^T$ . The 2 × 2 variance matrix $\hat{\bf{V}}$ is easily obtained from the 3 × 3 matrix $\bf{V}$ by skipping the third row and column:</p>
<script type="math/tex; mode=display">
\mathbf{V}=\left( \begin{array} {c c c} a & b & c \\ b & d & e \\ c & e & f \\ \end{array} \right) \Leftrightarrow\left( \begin{array} {c c} a & b \\ b & d \\ \end{array} \right)=\hat{\mathbf{V}}. \tag{13}</script><h3 id="The-Viewing-Transformation"><a href="#The-Viewing-Transformation" class="headerlink" title="The Viewing Transformation"></a>The Viewing Transformation</h3><p>object space, which has coordinates $\bf{t}=(t_0,t_1,t_2)^T$ ,the Gaussian reconstruction kernels in object space:</p>
<script type="math/tex; mode=display">
r_{k}^{\prime\prime} ( \mathbf{t} ) \;=\; \mathcal{G}_{\mathbf{V}_{k}^{\prime\prime}} ( \mathbf{t} \,-\, \mathbf{t}_{k} )</script><p>camera coordinates : $\bf{u} =(u_0,u_1,u_2)^T$,Object coordinates are transformed to camera coordinates using an affine mapping $\bf{u} = φ(\bf{t})$, called viewing transformation:</p>
<script type="math/tex; mode=display">
\varphi( \mathbf{t} )=\mathbf{W} \mathbf{t}+\mathbf{d}</script><p>We transform the reconstruction kernels $\mathcal{G}_{V^{′′}_k} (\bf{t} − \bf{t}_k)$to camera space by substituting $\bf{t} = φ^{−1}(\bf{u})$ and using Equation (10):</p>
<script type="math/tex; mode=display">
\mathcal{G}_{\mathbf{V}_{k}^{\prime\prime}} ( \varphi^{-1} ( \mathbf{u} )-\mathbf{t}_{k} )=\frac{1} {| \mathbf{W}^{-1} |} \mathcal{G}_{\mathbf{V}_{k}^{\prime}} ( \mathbf{u}-\mathbf{u}_{k} )=r_{k}^{\prime} ( \mathbf{u} ), \tag{14}</script><p>where$\bf{u}_k = φ(\bf{t}_k)$ is the center of the Gaussian in camera coordinates and $r^{‘} _k(\bf{u})$ denotes the reconstruction kernel in camera space.the variance matrix in camera coordinates $V^{′}_k$ is given by $\bf{V}^{′}_k = \bf{WV}^{′′}_k\bf{W}^T$ .</p>
<h3 id="The-Projective-Transformation"><a href="#The-Projective-Transformation" class="headerlink" title="The Projective Transformation"></a>The Projective Transformation</h3><p>Camera space and ray space are related by the mapping $\bf{x = m(u)}$:</p>
<script type="math/tex; mode=display">
\left( \begin{array} {c} x_{0} \\ x_{1} \\ x_{2} \\ \end{array} \right) \ \ =\ \ \mathbf{m} ( \mathbf{u} )=\left( \begin{array} {c} u_{0} / u_{2} \\ u_{1} / u_{2} \\ \| ( u_{0}, u_{1}, u_{2} )^{T} \| \\ \end{array} \right) \tag{15}</script><script type="math/tex; mode=display">
\left( \begin{array} {c} u_{0} \\ u_{1} \\ u_{2} \\ \end{array} \right) \ \ =\ \ \mathbf{m}^{-1} ( \mathbf{x} )=\left( \begin{array} {c} x_{0} / l \cdot x_{2}\\ x_{1} / l \cdot x_{2} \\ 1 / l \cdot x_{2} \\ \end{array} \right), \tag{16}</script><p>where $ l=|( x_{0}, x_{1}, 1 )^{T} |$.</p>
<p>the local affine approximation $\bf{m}_{u_{k}}$ of the projective transformation. It is defined by the first two terms of the Taylor expansion of $\bf{m}$ at the point $\bf{u}_k$:</p>
<script type="math/tex; mode=display">
\mathbf{m}_{\mathbf{u}_{k}} ( \mathbf{u} )=\mathbf{x}_{k}+\mathbf{J}_{\mathbf{u}_{k}} \cdot( \mathbf{u}-\mathbf{u}_{k} ), \tag{17}</script><script type="math/tex; mode=display">
\mathbf{J}_{\mathbf{u}_{k}}=\frac{\partial\mathbf{m}} {\partial\mathbf{u}} ( \mathbf{u}_{k} ). \tag{18}</script><p>apply Equation (10)</p>
<script type="math/tex; mode=display">
\begin{array} {r c l} r_{k} ( \mathbf{x} ) & = & \frac{1} {| \mathbf{W}^{-1} |} \mathcal{G}_{\mathbf{V}_{k}^{\prime}} ( \mathbf{m}^{-1} ( \mathbf{x} )-\mathbf{u}_{k} ) \\  & = & \frac{1} {| \mathbf{W}^{-1} | | \mathbf{J}^{-1} |} \mathcal{G}_{\mathbf{V}_{k}} ( \mathbf{x}-\mathbf{x}_{k} ), \\ \end{array} \tag{19}</script><script type="math/tex; mode=display">
\begin{array} {r c l} \bf {V}_{k} & = & \bf {J} \bf {V}_{k}^{\prime} \, \bf {J}^{T} \\  & = & \bf {J} {\bf W} {\bf V}_{k}^{\prime\prime} {\bf W}^{T} {\bf J}^{T}. \\ \end{array} \tag{20}</script><p><img src="/img/EWA3.png" alt="EWA3" title="EWA3"></p>
<p><img src="/img/EWA4.png" alt="EWA4" title="EWA4"></p>
<h3 id="Integration-and-Band-Limiting"><a href="#Integration-and-Band-Limiting" class="headerlink" title="Integration and Band-Limiting"></a>Integration and Band-Limiting</h3><p>Gaussian footprint function $q_k$:</p>
<script type="math/tex; mode=display">
\begin{array} {r c l} q_{k} ( \hat{\mathbf{x}} ) & = & \int_{\mathbb{R}} \frac{1} {| \mathbf{J}^{-1} | | \mathbf{W}^{-1} |} \mathcal{G}_{\mathbf{V}_{k}} ( \hat{\mathbf{x}}-\hat{\mathbf{x}}_{k}, x_{2}-x_{k 2} ) \, d x_{2} \\  & = & \frac{1} {| \mathbf{J}^{-1} | | \mathbf{W}^{-1} |} \mathcal{G}_{\hat{\mathbf{V}}_{k}} ( \hat{\mathbf{x}}-\hat{\mathbf{x}}_{k} ), \\ \end{array}\tag{21}</script><p>Gaussian low-pass filter:</p>
<script type="math/tex; mode=display">
h ~=~ {\mathcal G}_{\mathbf V^{h}} ( \hat{\mathbf x} )</script><p>EWA volume resampling filter:</p>
<script type="math/tex; mode=display">
\begin{array} {r c l} \rho_{k} ( \hat{\mathbf{x}} ) & = & ( q_{k} \otimes h ) ( \hat{\mathbf{x}} ) \\  & = & \frac{1} {| \mathbf{J}^{-1} | | \mathbf{W}^{-1} | |} ( \mathcal{G}_{\hat{\mathbf{V}}_{k}} \otimes\mathcal{G}_{\mathbf{V}^{h}} ) ( \hat{\mathbf{x}}-\hat{\mathbf{x}}_{k} ) \\  & = & \frac{1} {| \mathbf{J}^{-1} | | \mathbf{W}^{-1} |} \mathcal{G}_{\hat{\mathbf{V}}_{k}+\mathbf{V}^{h}} ( \hat{\mathbf{x}}-\hat{\mathbf{x}}_{k} ). \\ \end{array} \tag{22}</script><h3 id="Reduction-from-Volume-to-Surface-Reconstruction-Kernels"><a href="#Reduction-from-Volume-to-Surface-Reconstruction-Kernels" class="headerlink" title="Reduction from Volume to Surface Reconstruction Kernels"></a>Reduction from Volume to Surface Reconstruction Kernels</h3><p><img src="/img/EWA5.png" alt="EWA5" title="EWA5"></p>
<script type="math/tex; mode=display">
{\bf V}^{\prime\prime}=\left( \begin{array} {c c c} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & \frac{1} {s^{2}} \\ \end{array} \right).</script><script type="math/tex; mode=display">
\mathbf{V}=\mathbf{J} \mathbf{W} \mathbf{V}^{\prime\prime} \mathbf{W}^{T} \mathbf{J}^{T}=\mathbf{T}^{3 D} \mathbf{V}^{\prime\prime} \mathbf{T}^{3 D^{T}}.</script><script type="math/tex; mode=display">
\begin{array} {l} v_{0 0}=t_{0 0}^{2}+t_{0 1}^{2}+\frac{t_{0 2}^{2}} {s^{2}} \\ v_{0 1}=v_{1 0}=t_{0 0} t_{1 0}+t_{0 1} t_{1 1}+\frac{t_{0 2} t_{1 2}} {s^{2}} \\ v_{0 2}=v_{2 0}=t_{0 0} t_{2 0}+t_{0 1} t_{2 1}+\frac{t_{0 2} t_{2 2}} {s^{2}} \\ v_{1 1}=t_{1 0}^{2}+t_{1 1}^{2}+\frac{t_{1 2}^{2}} {s^{2}} \\ v_{2 2}=v_{2 1}=t_{1 0} t_{2 0}+t_{1 1} t_{2 1}+\frac{t_{1 2} t_{2 2}} {s^{2}} \\ v_{2 2}=t_{2 0}^{2}+t_{3 1}^{2}+\frac{t_{2 2}^{2}} {s^{2}}, \\ \end{array}</script><p>As s approaches infinity, we therefore get the following 2D variance matrix $\hat{\bf{V}}$:</p>
<script type="math/tex; mode=display">
\hat{\bf V}=\left( \begin{array} {c c} t_{0 0}^{2}+t_{0 1}^{2} & t_{0 0} t_{1 0}+t_{0 1} t_{1 1} \\ t_{0 0} t_{1 0}+t_{0 1} t_{1 1} & t_{1 0}^{2}+t_{1 1}^{2} \\ \end{array} \right). \tag{23}</script><script type="math/tex; mode=display">
\hat{\bf V}={\bf T}^{2 D} {\bf T}^{2 D^{T}}=\left( \begin{array} {c c} t_{0 0} & t_{0 1} \\ t_{1 0} & t_{1 1} \\ \end{array} \right) \left( \begin{array} {c c} t_{0 0} & t_{1 0} \\ t_{0 1} & t_{1 1} \\ \end{array} \right). \tag{24}</script><script type="math/tex; mode=display">
{\bf J}=\left( \begin{array} {c c c} 1 / u_{2} & 0 & -u_{0} / u_{2}^{2} \\ 0 & 1 / u_{2} & -u_{1} / u_{2}^{2} \\ u_{0} / l^{\prime} & u_{1} / l^{\prime} & u_{2} / l^{\prime} \\ \end{array} \right), \tag{25}</script><script type="math/tex; mode=display">
{\bf T}^{2 D}=\left( \begin{array} {c c c} 1 / u_{2} & 0 & -u_{0} / u_{2}^{2} \\ 0 & 1 / u_{2} & -u_{1} / u_{2}^{2} \\ \end{array} \right) \left( \begin{array} {c c} w_{0 0} & w_{0 1} \\ w_{1 0} & w_{1 1} \\ w_{2 0} & w_{2 1}\\ \end{array} \right)</script><p><img src="/img/EWA6.png" alt="EWA6" title="EWA6"></p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/">三维重建</a><a class="post-meta__tags" href="/tags/3D%E9%AB%98%E6%96%AF%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/">3D高斯预备知识</a></div><div class="post-share"><div class="social-share" data-image="/img/EWA1.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/wechat.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/12/10/cvaa2-1/" title="[计算机视觉：算法和应用]第二章：图像形成——2.1 几何图元与变换"><img class="cover" src="/img/cvaacover.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">[计算机视觉：算法和应用]第二章：图像形成——2.1 几何图元与变换</div></div><div class="info-2"><div class="info-item-1">原书PDF链接：Computer Vision: Algorithms and Applications, 2nd ed. 2.1 几何图元与变换​       在这一节将介绍这本书中所用到的基础的二维和三维图元，即点、线、面，也将描述三维特征是如何投影到二维特征。 有关这些话题更细致的描述可以在关于多视图几何的教科书中找到 《Multiple View Geometry in Computer Vision Second Edition》：https://assets.cambridge.org/97805215/40513/frontmatter/9780521540513_frontmatter.pdf 《The Geometry of Multiple...</div></div></div></a><a class="pagination-related" href="/2024/12/08/NeRF/" title="NeRF: Representing Scenes as  Neural Radiance Fields for View Synthesis"><img class="cover" src="/img/NeRF.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">NeRF: Representing Scenes as  Neural Radiance Fields for View Synthesis</div></div><div class="info-2"><div class="info-item-1">Neural Radiance Field Scene Representation输入：连续的5D坐标（空间位置+视角方向）  3 \mathrm{D ~ l o c a t i o n ~} \mathbf{x}=( x, y, z ) \mathrm{2 D ~ v i e w i n g ~ d i r e c t i o n ~} ( \theta, \phi)输出：空间位置上的体密度和辐射强度  \mathrm{emitted color}~{\bf c}=( r, g, b ) \mathrm{v o l u m e \ d e n s i t y} \ \sigma密度控制了穿过某一位置的光线在那个位置累积了多少辐射 方法简单总结： 1）将相机光轴穿过场景生成采样的三维点集 2）用三维点和对应的二维视角方向输入神经网络生成颜色和密度集  F_{\Theta} : ( {\bf x}, {\bf d} ) \to( {\bf c},...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/12/08/3DGS/" title="DIFFERENTIABLE 3D GAUSSIAN SPLATTING"><img class="cover" src="/img/3DGS1.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="fas fa-history fa-fw"></i> 2024-12-08</div><div class="info-item-2">DIFFERENTIABLE 3D GAUSSIAN SPLATTING</div></div><div class="info-2"><div class="info-item-1"> 3D高斯的优势：非结构化、可微、利用快速α混合进行渲染、无需法线 世界坐标系下，高斯由三维协方差矩阵和点（均值）μ表示：  G ( x ) \,=e^{-\frac{1} {2} ( x )^{T} \Sigma^{-1} ( x )} \tag{4}给定视角变换矩阵W，相机坐标系下的协方差矩阵为：  \Sigma^{\prime}=J W \Sigma W^{T} J^{T} \tag{5}其中J是投影变换的仿射近似雅可比矩阵 协方差矩阵只有在半正定的时候才具有物理意义，而如果直接对协方差矩阵使用梯度下降优化，很难保证矩阵的合理性。 由于协方差矩阵是用来描述椭球的形状，因此可以用缩放矩阵S和旋转矩阵R来获得一个对应的协方差矩阵：  \Sigma=R S S^{T} R^{T}\tag{6}OPTIMIZATION WITH ADAPTIVE DENSITY CONTROL OF 3D GAUSSIANSOptimization优化的参数：位置p，α，协方差矩阵，球谐函数系数  \mathcal{L}=( 1-\lambda)...</div></div></div></a><a class="pagination-related" href="/2024/12/06/Dust3r/" title="DUSt3R: Geometric 3D Vision Made Easy"><img class="cover" src="/img/Dust3r1.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="fas fa-history fa-fw"></i> 2024-12-08</div><div class="info-item-2">DUSt3R: Geometric 3D Vision Made Easy</div></div><div class="info-2"><div class="info-item-1">相关概念Pointmap将三维点的稠密二维场表示为pointmap $X \in\mathbb{R}^{W \times H \times3}$ ，对应分辨率为$W×H$ 的RGB图像 $I$，$X$描述了图像像素和三维场景点之间的一一对应关系。 假设每一条相机光线只会击中一个三维点，即忽略半透明表面的情况 Cameras and scene给定相机内参矩阵$K \in\mathbb{R}^{3 \times3}$ ，pointmap就能够根据给定的ground-truth 深度图$D \in\mathbb{R}^{W \times H}$ 获得：  X_{i, j}=K^{-1} D_{i, j} \left[ i, j, 1 \right]^{\top}$X^{n,m}$表示相机$n$的pointmap $X^n$在相机$m$坐标系中的表示：  X^{n, m}=P_{m} P_{n}^{-1} h \left( X^{n} \right) \tag{1}其中$P{m}, P{n} \in\mathbb{R}^{3 \times4}$ 是世界坐标系到相机坐标系的位姿，$h...</div></div></div></a><a class="pagination-related" href="/2024/12/08/MASt3R/" title="Grounding Image Matching in 3D with MASt3R"><img class="cover" src="/img/MASt3R1.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="fas fa-history fa-fw"></i> 2024-12-08</div><div class="info-item-2">Grounding Image Matching in 3D with MASt3R</div></div><div class="info-2"><div class="info-item-1">问题描述给定两张图像 $I^1$ 和 $I^2$ ，分别由未知参数的相机 $C^1$ 和 $C^2$ 采集，恢复出一组像素对应点 ${(i,j)}$ Method 由于gt pointmaps是metric的，所以为了得到metric的预测，将原本的nomalize的因子设置为 $z:= \hat{z}$ Matching prediction head and loss回归的方法固有地会受到噪声地影响，且DUSt3R没有显式地针对matching任务做训练。 Matching head  D^{1}=\mathrm{H e a d}_{\mathrm{d e s c}}^{1} ( [ H^{1}, H^{\prime1} ] ), \tag{8} D^{2}=\mathrm{H e a d}_{\mathrm{d e s c}}^{2} ( [ H^{2}, H^{\prime2} ] ). \tag{9}Head是一个简单的两层MLP以及非线性GELU激活函数，最后，将每个局部feature 归一化。 Matching objective 一张图像中的每一个local...</div></div></div></a><a class="pagination-related" href="/2024/12/08/Mip-Splatting/" title="Mip-Splatting: Alias-free 3D Gaussian Splatting"><img class="cover" src="/img/Mip-Splatting1.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="fas fa-history fa-fw"></i> 2024-12-08</div><div class="info-item-2">Mip-Splatting: Alias-free 3D Gaussian Splatting</div></div><div class="info-2"><div class="info-item-1">3DGS在改变采样率或者改变焦距和相机距离时，会存在严重的膨胀效应和高频伪影 采样理论（奈奎斯特-香农采样理论）对连续信号的采样频率至少是最大频率的两倍，所以在采样之前要对信号施加滤波。 3DGS中的Dilation操作为了防止投影的2DGS小于一个像素，所以投影的2DGS进行了膨胀操作：  {\mathcal G}_{k}^{2 D} ( \mathbf x )=e^{-{\frac{1} {2}} ( \mathbf x-\mathbf p_{k} )^{T}} ( \mathbf\Sigma_{k}^{2 D}+s \, \mathbf I )^{-1} \, ( \mathbf x-\mathbf p_{k} ) \tag{5}Sensitivity to Sampling Rate  对于Zoom-out，原本的object在像素中的占比缩小了，所以为了要膨胀到一个像素的大小，施加的二维膨胀相较于原来更大 对于Zoom-out，原本的object在像素中的占比变大，所以膨胀到一个像素大小所需的二维膨胀就更小 it leads to erosion effects...</div></div></div></a><a class="pagination-related" href="/2024/12/08/NeRF/" title="NeRF: Representing Scenes as  Neural Radiance Fields for View Synthesis"><img class="cover" src="/img/NeRF.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="fas fa-history fa-fw"></i> 2024-12-08</div><div class="info-item-2">NeRF: Representing Scenes as  Neural Radiance Fields for View Synthesis</div></div><div class="info-2"><div class="info-item-1">Neural Radiance Field Scene Representation输入：连续的5D坐标（空间位置+视角方向）  3 \mathrm{D ~ l o c a t i o n ~} \mathbf{x}=( x, y, z ) \mathrm{2 D ~ v i e w i n g ~ d i r e c t i o n ~} ( \theta, \phi)输出：空间位置上的体密度和辐射强度  \mathrm{emitted color}~{\bf c}=( r, g, b ) \mathrm{v o l u m e \ d e n s i t y} \ \sigma密度控制了穿过某一位置的光线在那个位置累积了多少辐射 方法简单总结： 1）将相机光轴穿过场景生成采样的三维点集 2）用三维点和对应的二维视角方向输入神经网络生成颜色和密度集  F_{\Theta} : ( {\bf x}, {\bf d} ) \to( {\bf c},...</div></div></div></a><a class="pagination-related" href="/2025/04/02/VGGT/" title="VGGT: Visual Geometry Grounded Transformer"><img class="cover" src="/img/VGGT-0.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="fas fa-history fa-fw"></i> 2025-04-10</div><div class="info-item-2">VGGT: Visual Geometry Grounded Transformer</div></div><div class="info-2"><div class="info-item-1">摘要三维计算机视觉通常被约束在单个任务中，因此我们提出了VGGT，一个前馈神经网络，直接推理场景的所有三维属性，包括相机参数，点云，深度图和三维点轨迹。同时该方法简单且效率高，可以在一秒内重建图像。 简介传统的三维重建任务采用视觉几何方法，但是会增加求解的复杂性和计算成本。DUSt3R等方法虽然能直接使用一个神经网络实现三维任务，但只能接受两个图像的输入，需要后处理来重建更多的图像。 VGGT不需要特定的网络，使用的是标准的transformer结构，在大规模公开数据集上训练。尽管存在潜在的冗余，但学习预测这些相互关联的3D属性可以提高整体准确性。在推理过程中，我们可以从单独预测的深度和相机参数中推导出点云，相比使用点云head可以得到更高的精度。 方法 问题定义和符号输入是$N$张图像$I_i \in \mathbb{R}^{3 \times H \times W}$ 的序列$(I_i)^N_{i=1}$，VGGT transofrmer将序列映射为对应的三维注释：  f \left( (I_i)_{i=1}^N \right) = (\mathbf{g}_i, D_i,...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/myphoto.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Zhu Jiajun</div><div class="author-info-description">North-Western polytechnical University</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">58</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">37</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Zjj-Low-Key"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/Zjj-Low-Key" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:zhujiajun.npu@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #24292e;"></i></a><a class="social-icon" href="/img/qq.jpg" target="_blank" title="QQ"><i class="fa-brands fa-qq" style="color: #24292e;"></i></a><a class="social-icon" href="/img/weixin.jpg" target="_blank" title="weixin"><i class="fa-brands fa-weixin" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">记录读研过程中学习阅读的相关论文书籍与研究内容。部分前期论文阅读记录格式较乱，请多多包涵。后期会尽量统一论文阅读记录的格式。同时也将更新更多有趣的内容。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Splatting-Algorithms"><span class="toc-number">1.</span> <span class="toc-text">Splatting Algorithms</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-EWA-Volume-Resampling-Filter"><span class="toc-number">2.</span> <span class="toc-text">The EWA Volume Resampling Filter</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Aliasing-in-Volume-Splatting"><span class="toc-number">2.1.</span> <span class="toc-text">Aliasing in Volume Splatting</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Elliptical-Gaussian-Kernels"><span class="toc-number">2.2.</span> <span class="toc-text">Elliptical Gaussian Kernels</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#The-Viewing-Transformation"><span class="toc-number">2.3.</span> <span class="toc-text">The Viewing Transformation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#The-Projective-Transformation"><span class="toc-number">2.4.</span> <span class="toc-text">The Projective Transformation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Integration-and-Band-Limiting"><span class="toc-number">2.5.</span> <span class="toc-text">Integration and Band-Limiting</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reduction-from-Volume-to-Surface-Reconstruction-Kernels"><span class="toc-number">2.6.</span> <span class="toc-text">Reduction from Volume to Surface Reconstruction Kernels</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/07/04/procdeptheval/" title="Evaluating Robustness of Monocular Depth Estimation with Procedural Scene Perturbations"><img src="/img/proc-0.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Evaluating Robustness of Monocular Depth Estimation with Procedural Scene Perturbations"/></a><div class="content"><a class="title" href="/2025/07/04/procdeptheval/" title="Evaluating Robustness of Monocular Depth Estimation with Procedural Scene Perturbations">Evaluating Robustness of Monocular Depth Estimation with Procedural Scene Perturbations</a><time datetime="2025-07-04T02:34:38.000Z" title="发表于 2025-07-04 10:34:38">2025-07-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/07/03/OmniStereo/" title="OmniStereo: Real-time Omnidireactional Depth Estimation with Multiview  Fisheye Cameras"><img src="/img/OmniStereo-0.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="OmniStereo: Real-time Omnidireactional Depth Estimation with Multiview  Fisheye Cameras"/></a><div class="content"><a class="title" href="/2025/07/03/OmniStereo/" title="OmniStereo: Real-time Omnidireactional Depth Estimation with Multiview  Fisheye Cameras">OmniStereo: Real-time Omnidireactional Depth Estimation with Multiview  Fisheye Cameras</a><time datetime="2025-07-03T01:42:30.000Z" title="发表于 2025-07-03 09:42:30">2025-07-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/04/02/VGGT/" title="VGGT: Visual Geometry Grounded Transformer"><img src="/img/VGGT-0.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="VGGT: Visual Geometry Grounded Transformer"/></a><div class="content"><a class="title" href="/2025/04/02/VGGT/" title="VGGT: Visual Geometry Grounded Transformer">VGGT: Visual Geometry Grounded Transformer</a><time datetime="2025-04-02T12:20:27.000Z" title="发表于 2025-04-02 20:20:27">2025-04-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/29/amodeldepthanything/" title="Amodal Depth Anything: Amodal Depth Estimation in the Wild"><img src="/img/amodeldepthanything-0.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Amodal Depth Anything: Amodal Depth Estimation in the Wild"/></a><div class="content"><a class="title" href="/2025/03/29/amodeldepthanything/" title="Amodal Depth Anything: Amodal Depth Estimation in the Wild">Amodal Depth Anything: Amodal Depth Estimation in the Wild</a><time datetime="2025-03-29T00:31:20.000Z" title="发表于 2025-03-29 08:31:20">2025-03-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/03/26/depthprompting/" title="Depth Prompting for Sensor-Agnostic Depth Estimation"><img src="/img/depthprompting-0.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Depth Prompting for Sensor-Agnostic Depth Estimation"/></a><div class="content"><a class="title" href="/2025/03/26/depthprompting/" title="Depth Prompting for Sensor-Agnostic Depth Estimation">Depth Prompting for Sensor-Agnostic Depth Estimation</a><time datetime="2025-03-26T05:53:11.000Z" title="发表于 2025-03-26 13:53:11">2025-03-26</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/EWA1.png);"><div id="footer-wrap"><div class="copyright">&copy;2024 - 2025 By Zhu Jiajun</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my blog !</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>